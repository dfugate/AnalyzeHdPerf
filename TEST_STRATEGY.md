# Test Strategy

## Overview and Scope
This is the testing strategy for the *Hard Drive Performance Analyzer* tool. This
document shall be used to guide how testing will be managed for this project.
The test effort will be prioritized and executed based on the project's
priorities. This is a living document that may be refined as the project
progresses. The scope of this document is limited to *Hard Drive Performance
Analyzer* testing strategies within the supported environment and on supported
platofrms.

## Test Approach

### Unit
Unit testing is testing performed to determine that individual program modules
perform per the design specifications.

In this case, a new Python package, *perf_analyzer_test*, will be created and
include modules named similarly to what's found in the *perf_analyzer* package.
Said modules will include test classes dependent upon the *unittest* module and
will hit all Python classes, functions, etc. residing in the
*perf_analyzer* module. This will be done for both positive and negative
scenarios. Said tests will utilize Python's
[mock](https://pypi.python.org/pypi/mock) API (as appropriate) to fake execution
of clients on remote machines (amongst other things). Finally, a test helper
module, *test_utils*, will be authored to provide common functionality amongst
all unit tests.

### System
System testing is the process of testing an integrated system to verify that
it meets specified requirements. This testing will determine if the results
generated by information systems and their components are accurate and that the
system performs according to specifications.

System testing will be accomplished in two ways:
* a Bash script, *sanity_check.sh*, will be created to run the
server/client on a single host and automatically validate it's output as being
reasonable.
* a test plan will be written including test cases to be manually
verified (e.g., checking the output of the various HTML views of the
benchmarking data). While it's best that a human-being verify the output of
the server, kicking off these distinct test cases will be delegated to another
Bash script, *system_tests.sh*.

#### Sample System Test Cases
* **Positive** - server is started with no flags.
**Expectation** - three local clients are started and finish successfully.
* **Negative** - server is started with no flags and the *bottle* package
has not been installed.
**Expectation** - the server reports that *bottle* hasn't been installed and
fails to start.
* **Positive** - *--client* flag is used to supply the name of a remote client
with all software prerequisites met.
**Expectation** - client runs successfully and reports back to the server.
* **Negative** -  *--client* flag is used to supply the name of a remote client
where software prerequisites are not met.
**Expectation** - server reports the client as *Dead*.
* **Negative** - *--client* flag is used to supply the name of a non-existent
client.
 **Expectation** - server reports the client as *Dead*.
* Etc.

### Usability
Usability testing tests the ease with which users can learn and use a product.

This aspect of testing will be primarily covered by ignoring all internal
knowledge of this project, and trying to install and run it exclusively by
the information provided in the README. Similarly, the HTML views of the data
will be presented to an end-user who has never used this application before,
and their feedback will be solicited.

Another aspect of usability is how this web app performs against different
web browsers, varying screen resolutions, screen readers, and also
how it's viewed by the color-blind.

### Load
Load testing simulates multi-user or multi-threaded access to an
application or module to ensure that components and databases can be used to
perform specified requirements with no catastrophic failures.

For this web-app, three major load scenarios will be explored:
* many different (remote and local) clients connected to a single server.
* few benchmark clients, but many, many web browsers requesting HTML views
from a single server.
* few benchmark clients, but with a high frequency of heartbeat and resource
monitoring notifications.

### Performance
Performance testing is conducted to evaluate the compliance of a system or
componentâ€™s response time, and the ability to function in various operating
environments.

The performance testing effort will principally be a manual effort. In
particular, web server response and page load times on the server will be
scrutinized to ensure they're reasonable (e.g., page loads in under five
seconds).

### Regression
Regression testing involves re-testing a previously tested program following
modification to ensure that faults have not been introduced or uncovered as a
result of the changes made. In this release this will be covered by the ongoing
use of manual tests being executed after each successful build of the
application, prior to release of the build for general testing use.

N/A. This web app hasn't been released yet.

### Recovery
Recovery testing forces the failure of the software in a variety of ways to
verify that recovery is properly performed.

N/A (by-design). This web app doesn't make use of a database to store results
(possible future feature?). Everything's held in memory currently.

### Security
Security testing evaluates whether the system meets its specified security objectives by attempting to break in or
disable a system by improper acquisition of a password, bypassing security measures, browsing through insecure data,
or overwhelming the system with requests.

This web app is designed to be Intranet-only => security scenarios are less
interesting. From the client perspective, it might be worthwhile to see what
occurs when the pre-defined directory to execute the tests within can't be
written to due to security permissions. As for the web app, interesting
scenarios include:
* attempting to access the website as another user.
* attempting to start clients outside the Intranet.

### Installation and Configuration
Installation / Configuration testing verifies that the system will install and
function on all required operating platforms, under all specified configurations.
These test platforms are described below.

#### Supported OSes
* Mac OS 10 (or higher)
* Ubuntu 14 (or higher)

#### Supported Web Browsers
* Chrome 40 (or higher)
* FireFox 40 (or higher)
* Safari 8 (or higher - Mac-only)

#### Minimum Hardware
* Intel Dual Core 2.6 GHz
* 2 GB Memory
* Network Connection
* 25 MB Free Disk Space (server)
* 1 GB Free Disk Space* (client)

#### Documentation Verification
Documentation verification involves reviewing for accuracy all supporting User
Documentation, Help Files, and supplemental materials.

All documents will be reviewed prior to release.

### Code Coverage
A block code coverage tool will be utilized in conjunction with the unit tests
to ensure that > 85% of all code in the *perf_analyzer* package is hit.

## Release Control
No new release of this web app is to be created without passing > 95% of the
unit tests and 100% of system tests.

## Risks and Assumptions
* this web app automatically determines *chunk size* based on the total number
of clients it handles. I.e., chunk size grows exponentially based on the
number of clients which implies there's a relatively low number of total clients
the server can handle on any given run. This limits the load testing scenarios.
* clients assume a certain directory they run the benchmarks from will always be
local to the machine. This may not always be the case.
* OS-level updates to the *ssh* tool can affect supported command-line arguments
which can in turn break this tool.
* if this application and tests are run against a virtual operating system,
there's a definite possibility that the performance metrics could be incorrect.
* the ability for a client to refuse to run a benchmark in which it can't handle
at least two file rollovers is based on estimated disk-write throughput and
can be inaccurate due to the overhead of other processes.
* setting up a UNIX box to accept remote HTTP requests on port 8080 (default TCP
port for this web app) can be challenging.