# Test Strategy

## Scope and Overview
This is the testing strategy for the *Hard Drive Performance Analyzer* tool. This
document shall be used to guide how testing will be managed for this project.
The test effort will be prioritized and executed based on the project's
priorities. This is a living document that may be refined as the project
progresses.

## Test Approach

### Unit
Unit testing is testing performed to determine that individual program modules
perform per the design specifications.

In this case, a new Python package, *perf_analyzer_test*, will be created and
include modules named similarly to what's found in the *perf_analyzer* package.
Said modules, will include test classes derived from the *unittest* module and
will hit all Python classes, functions, etc. residing in the corresponding
*perf_analyzer* module. This will be done for both positive and negative
scenarios. Said tests will utilize Python's
[mock](https://pypi.python.org/pypi/mock) API as-appropriate to fake execution
of clients on remote machines (amongst other things). Finally, a test helper
module, *test_utils*, will be authored to provide common functionality amongst
the unit tests.

### System
System testing is the process of testing an integrated system to verify that
it meets specified requirements. This testing will determine if the results
generated by information systems and their components are accurate and that the
system performs according to specifications.

System testing will be accomplished in two distinct manners:
* a Bash script, *sanity_check.sh*, will be created to run the
server/client on a single host and automatically validate it's output as being
reasonable.
* a test plan will be written including test cases to be manually
verified (e.g., checking the output of the various HTML views of the
benchmarking data). While it's best that a human-being verify the output of
the server, kicking off these distinct test cases will be delegated to another
Bash script, *system_tests.sh*.

### Usability
Usability testing tests the ease with which users can learn and use a product.

This aspect of testing will be primarily covered by ignoring all internal
knowledge of this project, and trying to install and run it exclusively by
the information provided in the README. Similarly, the HTML views of the data
will be presented to an end-user who has never used this application before,
and their feedback will be solicited.

Another aspect of usability is how this web app performs against different
web browsers, varying screen resolutions, against screen readers, and also
how it's viewed by the color-blind.

### Load
Load testing simulates multi-user or multi-threaded access to an
application or module to ensure that components and databases can be used to
perform specified requirements with no catastrophic failures.

For this web-app, three major load scenarios will be explored:
* many different (remote and local) clients connected to a single server.
* few benchmark clients, but many, many web browsers requesting HTML views
from a single server.
* few benchmark clients, but with a high frequency of heartbeat and resource monitoring notifications

### Performance
Performance testing is conducted to evaluate the compliance of a system or
componentâ€™s response time, and the ability to function in various operating
environments.

The performance testing effort will principally be a manual effort. In
particular, web server response times on the server will be scrutinized to
ensure they're reasonable (e.g., page loads in under five seconds).

### Regression
Regression testing involves re-testing a previously tested program following
modification to ensure that faults have not been introduced or uncovered as a
result of the changes made. In this release this will be covered by the ongoing
use of manual tests being executed after each successful build of the
application, prior to release of the build for general testing use.

N/A. This web app hasn't been released yet.

### Recovery
Recovery testing forces the failure of the software in a variety of ways to
verify that recovery is properly performed.

N/A (by-design). This web app doesn't make use of a database to store results
(possible future feature?). Everything's held in memory currently.

### Security
Security testing evaluates whether the system meets its specified security objectives by attempting to break in or
disable a system by improper acquisition of a password, bypassing security measures, browsing through insecure data,
or overwhelming the system with requests.

This web app is designed to be Intranet-only => security scenarios are less
interesting. From the client perspective, it might be interesting to see what
occurs when the pre-defined directory to execute the tests can't be written to
due to security permissions. As for the web app, interesting scenarios
include:
* attempting to access the website as another user.
* attempting to start clients outside the Intranet

### Installation and Configuration
Installation / Configuration testing verifies that the system will install and
function on all required operating platforms, under all specified configurations.
These test platforms are described below.

#### Supported OSes
* Mac OS 10 (or higher)
* Ubuntu 14 (or higher)

#### Supported Web Browsers
* Chrome 40 (or higher)
* FireFox 40 (or higher)
* Safari 8 (or higher - Mac-only)

#### Minimum Hardware
* Intel Dual Core 2.6 GHz
* 2 GB Memory
* Network Connection
* 25 MB Free Disk Space (server)
* 1 GB Free Disk Space* (client)

#### Documentation Verification
Documentation verification involves reviewing for accuracy all supporting User
Documentation, Help Files, and supplemental materials.

All documents will be reviewed prior to release.

### Code Coverage
A block code coverage tool will be utilized in conjunction with the unit tests
to ensure that > 85% of all code in the *perf_analyzer* package is hit.

## Release Control
No new release of this web app is to be created without passing > 95% of the
unit tests and all system tests.
